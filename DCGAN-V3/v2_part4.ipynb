{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc_r5G_-jX8y",
        "outputId": "a5ee46e1-62d9-4a8f-faf9-3d51660c9d47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FULL STAGED TRAINING\n",
            "Dataset: 8 messages\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STAGE 1: TRAINING ALICE+BOB FOR RECONSTRUCTION\n",
            "======================================================================\n",
            "Epoch   0 | Loss: 4.6122 | Accuracy: 0.0%\n",
            "Epoch  10 | Loss: 3.9068 | Accuracy: 14.1%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:1340: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  current = float(metrics)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  20 | Loss: 3.4374 | Accuracy: 16.7%\n",
            "Epoch  30 | Loss: 3.1812 | Accuracy: 11.1%\n",
            "Epoch  40 | Loss: 3.3778 | Accuracy: 19.7%\n",
            "Epoch  50 | Loss: 3.0628 | Accuracy: 18.2%\n",
            "Epoch  60 | Loss: 2.9363 | Accuracy: 19.4%\n",
            "Epoch  70 | Loss: 2.9831 | Accuracy: 21.2%\n",
            "Epoch  80 | Loss: 3.1010 | Accuracy: 27.1%\n",
            "Epoch  90 | Loss: 3.1500 | Accuracy: 23.3%\n",
            "\n",
            "Stage 1 Complete! Final accuracy: 22.4%\n",
            "WARNING: Bob accuracy < 90%. Consider training longer.\n",
            "\n",
            "======================================================================\n",
            "STAGE 2: TRAINING EVE (ADVERSARY)\n",
            "======================================================================\n",
            "Epoch   0 | Loss: 4.5683 | Eve Accuracy: 1.3%\n",
            "Epoch  10 | Loss: 4.0375 | Eve Accuracy: 16.9%\n",
            "Epoch  20 | Loss: 3.8339 | Eve Accuracy: 22.5%\n",
            "\n",
            "Stage 2 Complete! Eve trained.\n",
            "\n",
            "======================================================================\n",
            "STAGE 3: ADVERSARIAL FINE-TUNING\n",
            "======================================================================\n",
            "Epoch   0 | Bob: 3.153 (23.7%) | Eve: 3.508 (30.3%) | Ratio: 1.11x\n",
            "Epoch  10 | Bob: 3.021 (26.6%) | Eve: 3.157 (39.2%) | Ratio: 1.05x\n",
            "Epoch  20 | Bob: 3.158 (19.4%) | Eve: 3.066 (31.9%) | Ratio: 0.97x\n",
            "Epoch  30 | Bob: 2.906 (19.7%) | Eve: 2.526 (48.7%) | Ratio: 0.87x\n",
            "Epoch  40 | Bob: 3.044 (25.0%) | Eve: 2.504 (46.4%) | Ratio: 0.82x\n",
            "\n",
            "Stage 3 Complete!\n",
            "\n",
            "======================================================================\n",
            "FINAL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Example 1:\n",
            "  Original: 'Hello World!'\n",
            "  Bob:      'Te                                      ee            eee      ' (5.3%)\n",
            "  Eve:      'Teoro eoreo.' (41.7%)\n",
            "\n",
            "Example 2:\n",
            "  Original: 'This is a test.'\n",
            "  Bob:      'Tee              ee  e    e      e            e              e ' (12.8%)\n",
            "  Eve:      'Tuis is e tes.' (82.8%)\n",
            "\n",
            "Example 3:\n",
            "  Original: 'Secret message here.'\n",
            "  Bob:      'Te           e  e                  eee           e           e ' (9.6%)\n",
            "  Eve:      'Teoret  essste teoe.' (65.0%)\n",
            "\n",
            "Example 4:\n",
            "  Original: 'Encryption works!'\n",
            "  Bob:      'Tee                 e                      ee    e          e  ' (2.5%)\n",
            "  Eve:      'Te  ost oe  oo st' (29.4%)\n",
            "\n",
            "Example 5:\n",
            "  Original: 'Neural crypto system.'\n",
            "  Bob:      'Tee               e               e        e                e e' (9.5%)\n",
            "  Eve:      'Te reo ersrto syste.' (63.4%)\n",
            "\n",
            "======================================================================\n",
            "Bob Similarity: 8.0% ✗\n",
            "Eve Similarity: 56.5% ✗\n",
            "Security Ratio: 0.14x\n",
            "======================================================================\n",
            "\n",
            "✓ Training complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ============ Staged Training System ============\n",
        "class StagedCryptoTrainer:\n",
        "    \"\"\"\n",
        "    3-stage training approach:\n",
        "    Stage 1: Train Alice+Bob for perfect reconstruction (no Eve)\n",
        "    Stage 2: Train Eve to attack (freeze Alice+Bob)\n",
        "    Stage 3: Adversarial fine-tuning (all together)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=98, embed_dim=128, alice_layers=3,\n",
        "                 bob_layers=3, eve_layers=4, max_len=128,\n",
        "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Import components\n",
        "        from v2_part1_simplified import StringProcessor, KeyManager\n",
        "        from v2_part2_alice import AliceEncryptor\n",
        "        from v2_part3_bob_eve import BobDecryptor, EveAttacker\n",
        "\n",
        "        self.processor = StringProcessor(max_length=max_len)\n",
        "        self.key_manager = KeyManager(device=device)\n",
        "\n",
        "        # Initialize networks\n",
        "        self.alice_wrapper = AliceEncryptor(vocab_size, embed_dim, alice_layers, max_len, device)\n",
        "        self.bob_wrapper = BobDecryptor(vocab_size, embed_dim, bob_layers, max_len, device)\n",
        "        self.eve_wrapper = EveAttacker(vocab_size, embed_dim, eve_layers, max_len, device)\n",
        "\n",
        "        self.alice = self.alice_wrapper.alice\n",
        "        self.bob = self.bob_wrapper.bob\n",
        "        self.eve = self.eve_wrapper.eve\n",
        "\n",
        "        # Optimizers\n",
        "        self.opt_alice = optim.Adam(self.alice.parameters(), lr=0.001)\n",
        "        self.opt_bob = optim.Adam(self.bob.parameters(), lr=0.001)\n",
        "        self.opt_eve = optim.Adam(self.eve.parameters(), lr=0.0005)\n",
        "\n",
        "        # Schedulers\n",
        "        self.sched_alice = optim.lr_scheduler.ReduceLROnPlateau(self.opt_alice, patience=5, factor=0.5)\n",
        "        self.sched_bob = optim.lr_scheduler.ReduceLROnPlateau(self.opt_bob, patience=5, factor=0.5)\n",
        "        self.sched_eve = optim.lr_scheduler.ReduceLROnPlateau(self.opt_eve, patience=5, factor=0.8)\n",
        "\n",
        "        # Loss\n",
        "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "        self.current_stage = 1\n",
        "\n",
        "    def train_stage1_reconstruction(self, messages, num_epochs=50):\n",
        "        \"\"\"\n",
        "        Stage 1: Train Alice+Bob for perfect reconstruction\n",
        "        Goal: Bob accuracy > 90%\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"STAGE 1: TRAINING ALICE+BOB FOR RECONSTRUCTION\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        history = {'loss': [], 'accuracy': []}\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Sample batch\n",
        "            batch = np.random.choice(messages, size=min(4, len(messages)), replace=False).tolist()\n",
        "            tokens = self.processor.batch_encode(batch).to(self.device)\n",
        "\n",
        "            # Generate keys\n",
        "            keys = self.key_manager.generate_keys_for_batch(len(batch), tokens.size(1))\n",
        "\n",
        "            # Forward pass\n",
        "            self.opt_alice.zero_grad()\n",
        "            self.opt_bob.zero_grad()\n",
        "\n",
        "            self.alice.train()\n",
        "            self.bob.train()\n",
        "\n",
        "            encrypted = self.alice(tokens, keys['key_tensors'])\n",
        "            logits = self.bob(encrypted, keys['key_tensors'])\n",
        "\n",
        "            # Reconstruction loss\n",
        "            loss = self.ce_loss(logits.reshape(-1, logits.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.alice.parameters(), 1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(self.bob.parameters(), 1.0)\n",
        "\n",
        "            self.opt_alice.step()\n",
        "            self.opt_bob.step()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            with torch.no_grad():\n",
        "                pred_tokens = torch.argmax(logits, dim=-1)\n",
        "                mask = tokens != 0\n",
        "                correct = ((pred_tokens == tokens) & mask).sum().item()\n",
        "                total = mask.sum().item()\n",
        "                accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "            history['loss'].append(loss.item())\n",
        "            history['accuracy'].append(accuracy)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | Accuracy: {accuracy*100:.1f}%\")\n",
        "\n",
        "            # Update learning rate\n",
        "            self.sched_alice.step(loss)\n",
        "            self.sched_bob.step(loss)\n",
        "\n",
        "        final_acc = np.mean(history['accuracy'][-10:])\n",
        "        print(f\"\\nStage 1 Complete! Final accuracy: {final_acc*100:.1f}%\")\n",
        "\n",
        "        if final_acc > 0.90:\n",
        "            print(\"SUCCESS: Bob can decrypt accurately!\")\n",
        "            self.current_stage = 2\n",
        "        else:\n",
        "            print(\"WARNING: Bob accuracy < 90%. Consider training longer.\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    def train_stage2_adversary(self, messages, num_epochs=30):\n",
        "        \"\"\"\n",
        "        Stage 2: Train Eve to attack (freeze Alice+Bob)\n",
        "        Goal: Eve learns patterns but can't match Bob\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"STAGE 2: TRAINING EVE (ADVERSARY)\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Freeze Alice and Bob\n",
        "        for param in self.alice.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.bob.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        history = {'loss': [], 'accuracy': []}\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            batch = np.random.choice(messages, size=min(4, len(messages)), replace=False).tolist()\n",
        "            tokens = self.processor.batch_encode(batch).to(self.device)\n",
        "\n",
        "            keys = self.key_manager.generate_keys_for_batch(len(batch), tokens.size(1))\n",
        "\n",
        "            # Get encrypted data from Alice\n",
        "            with torch.no_grad():\n",
        "                encrypted = self.alice(tokens, keys['key_tensors'])\n",
        "\n",
        "            # Train Eve\n",
        "            self.opt_eve.zero_grad()\n",
        "            self.eve.train()\n",
        "\n",
        "            eve_logits = self.eve(encrypted)\n",
        "            loss = self.ce_loss(eve_logits.reshape(-1, eve_logits.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.eve.parameters(), 1.0)\n",
        "            self.opt_eve.step()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            with torch.no_grad():\n",
        "                pred_tokens = torch.argmax(eve_logits, dim=-1)\n",
        "                mask = tokens != 0\n",
        "                correct = ((pred_tokens == tokens) & mask).sum().item()\n",
        "                total = mask.sum().item()\n",
        "                accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "            history['loss'].append(loss.item())\n",
        "            history['accuracy'].append(accuracy)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | Eve Accuracy: {accuracy*100:.1f}%\")\n",
        "\n",
        "            self.sched_eve.step(loss)\n",
        "\n",
        "        # Unfreeze Alice and Bob\n",
        "        for param in self.alice.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.bob.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        print(f\"\\nStage 2 Complete! Eve trained.\")\n",
        "        self.current_stage = 3\n",
        "\n",
        "        return history\n",
        "\n",
        "    def train_stage3_adversarial(self, messages, num_epochs=50):\n",
        "        \"\"\"\n",
        "        Stage 3: Adversarial fine-tuning\n",
        "        Goal: Alice learns to fool Eve while Bob still decrypts\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"STAGE 3: ADVERSARIAL FINE-TUNING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        history = {\n",
        "            'bob_loss': [], 'eve_loss': [],\n",
        "            'bob_acc': [], 'eve_acc': [], 'ratio': []\n",
        "        }\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            batch = np.random.choice(messages, size=min(4, len(messages)), replace=False).tolist()\n",
        "            tokens = self.processor.batch_encode(batch).to(self.device)\n",
        "            keys = self.key_manager.generate_keys_for_batch(len(batch), tokens.size(1))\n",
        "\n",
        "            # Train Alice+Bob\n",
        "            self.opt_alice.zero_grad()\n",
        "            self.opt_bob.zero_grad()\n",
        "\n",
        "            self.alice.train()\n",
        "            self.bob.train()\n",
        "\n",
        "            encrypted = self.alice(tokens, keys['key_tensors'])\n",
        "            bob_logits = self.bob(encrypted, keys['key_tensors'])\n",
        "            bob_loss = self.ce_loss(bob_logits.reshape(-1, bob_logits.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            bob_loss.backward()\n",
        "            self.opt_alice.step()\n",
        "            self.opt_bob.step()\n",
        "\n",
        "            # Train Eve\n",
        "            self.opt_eve.zero_grad()\n",
        "            self.eve.train()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                encrypted_for_eve = self.alice(tokens, keys['key_tensors'])\n",
        "\n",
        "            eve_logits = self.eve(encrypted_for_eve)\n",
        "            eve_loss = self.ce_loss(eve_logits.reshape(-1, eve_logits.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            eve_loss.backward()\n",
        "            self.opt_eve.step()\n",
        "\n",
        "            # Adversarial update for Alice (make Eve fail)\n",
        "            self.opt_alice.zero_grad()\n",
        "\n",
        "            encrypted_adv = self.alice(tokens, keys['key_tensors'])\n",
        "            eve_logits_adv = self.eve(encrypted_adv)\n",
        "            adv_loss = -self.ce_loss(eve_logits_adv.reshape(-1, eve_logits_adv.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            (adv_loss * 0.5).backward()  # Weight adversarial loss lower\n",
        "            self.opt_alice.step()\n",
        "\n",
        "            # Calculate metrics\n",
        "            with torch.no_grad():\n",
        "                bob_pred = torch.argmax(bob_logits, dim=-1)\n",
        "                eve_pred = torch.argmax(eve_logits, dim=-1)\n",
        "                mask = tokens != 0\n",
        "\n",
        "                bob_acc = ((bob_pred == tokens) & mask).sum().item() / mask.sum().item()\n",
        "                eve_acc = ((eve_pred == tokens) & mask).sum().item() / mask.sum().item()\n",
        "                ratio = eve_loss.item() / (bob_loss.item() + 1e-8)\n",
        "\n",
        "            history['bob_loss'].append(bob_loss.item())\n",
        "            history['eve_loss'].append(eve_loss.item())\n",
        "            history['bob_acc'].append(bob_acc)\n",
        "            history['eve_acc'].append(eve_acc)\n",
        "            history['ratio'].append(ratio)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch:3d} | Bob: {bob_loss.item():.3f} ({bob_acc*100:.1f}%) | \"\n",
        "                      f\"Eve: {eve_loss.item():.3f} ({eve_acc*100:.1f}%) | Ratio: {ratio:.2f}x\")\n",
        "\n",
        "        print(f\"\\nStage 3 Complete!\")\n",
        "        return history\n",
        "\n",
        "    def full_training(self, messages, stage1_epochs=50, stage2_epochs=30, stage3_epochs=50):\n",
        "        \"\"\"Run all 3 stages\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FULL STAGED TRAINING\")\n",
        "        print(f\"Dataset: {len(messages)} messages\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        h1 = self.train_stage1_reconstruction(messages, stage1_epochs)\n",
        "        h2 = self.train_stage2_adversary(messages, stage2_epochs)\n",
        "        h3 = self.train_stage3_adversarial(messages, stage3_epochs)\n",
        "\n",
        "        return {'stage1': h1, 'stage2': h2, 'stage3': h3}\n",
        "\n",
        "\n",
        "# ============ Evaluation ============\n",
        "class CryptoEvaluator:\n",
        "    @staticmethod\n",
        "    def evaluate(trainer, test_messages):\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FINAL EVALUATION\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        results = {'bob_sim': [], 'eve_sim': [], 'examples': []}\n",
        "\n",
        "        for i, original in enumerate(test_messages[:5]):\n",
        "            tokens = trainer.processor.encode(original).unsqueeze(0).to(trainer.device)\n",
        "            keys = trainer.key_manager.generate_keys_for_batch(1, tokens.size(1))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                encrypted = trainer.alice(tokens, keys['key_tensors'])\n",
        "                bob_logits = trainer.bob(encrypted, keys['key_tensors'])\n",
        "                eve_logits = trainer.eve(encrypted)\n",
        "\n",
        "                bob_tokens = torch.argmax(bob_logits, dim=-1)\n",
        "                eve_tokens = torch.argmax(eve_logits, dim=-1)\n",
        "\n",
        "                bob_msg = trainer.processor.decode(bob_tokens[0])\n",
        "                eve_msg = trainer.processor.decode(eve_tokens[0])\n",
        "\n",
        "            bob_sim = SequenceMatcher(None, original, bob_msg).ratio()\n",
        "            eve_sim = SequenceMatcher(None, original, eve_msg).ratio()\n",
        "\n",
        "            results['bob_sim'].append(bob_sim)\n",
        "            results['eve_sim'].append(eve_sim)\n",
        "            results['examples'].append((original, bob_msg, eve_msg))\n",
        "\n",
        "            print(f\"\\nExample {i+1}:\")\n",
        "            print(f\"  Original: '{original}'\")\n",
        "            print(f\"  Bob:      '{bob_msg}' ({bob_sim*100:.1f}%)\")\n",
        "            print(f\"  Eve:      '{eve_msg}' ({eve_sim*100:.1f}%)\")\n",
        "\n",
        "        avg_bob = np.mean(results['bob_sim'])\n",
        "        avg_eve = np.mean(results['eve_sim'])\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"Bob Similarity: {avg_bob*100:.1f}% {'✓' if avg_bob > 0.9 else '✗'}\")\n",
        "        print(f\"Eve Similarity: {avg_eve*100:.1f}% {'✓' if avg_eve < 0.3 else '✗'}\")\n",
        "        print(f\"Security Ratio: {avg_bob/max(avg_eve,0.01):.2f}x\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# ============ Main ============\n",
        "if __name__ == \"__main__\":\n",
        "    # Dataset\n",
        "    messages = np.array([\n",
        "        \"Hello World!\",\n",
        "        \"This is a test.\",\n",
        "        \"Secret message here.\",\n",
        "        \"Encryption works!\",\n",
        "        \"Neural crypto system.\",\n",
        "        \"Testing ABC 123.\",\n",
        "        \"Quick brown fox.\",\n",
        "        \"The lazy dog jumps.\"\n",
        "    ])\n",
        "\n",
        "    # Initialize\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    trainer = StagedCryptoTrainer(\n",
        "        vocab_size=98,\n",
        "        embed_dim=128,\n",
        "        alice_layers=3,\n",
        "        bob_layers=3,\n",
        "        eve_layers=4,\n",
        "        max_len=64,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    history = trainer.full_training(\n",
        "        messages,\n",
        "        stage1_epochs=100,  # Focus on reconstruction\n",
        "        stage2_epochs=30,\n",
        "        stage3_epochs=50\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    results = CryptoEvaluator.evaluate(trainer, messages)\n",
        "\n",
        "    print(\"\\n✓ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_DICthTkDxh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}